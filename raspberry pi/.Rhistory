?rnorm
library(caret)
install.packages("caret")
library(caret)
library(ggplot2)
library(dummies)
install.packages("dummies")
library(dummies)
library(pROC)
install.packages("ROCR")
install.packages("pROC")
install.packages("randomForest")
install.packages("xgboost")
?lm
rm(list = ls())
library(MASS)
library(ISLR)
install.packages("ISLR")
install.packages("anomalyDetection")
install.packages("imageData")
install.packages("imager")
rm(list = ls())
install.packages("formatR")
install.packages("formattable")
install.packages("neural")
install.packages("neuralnet")
install.packages("NeuralNetTools")
rm(list = ls())
install.packages("arules")
install.packages("arulesViz")
?rnorm
x<-c(1,3,2,5)
x
x=c(1,6,2)
x
y=c(1,4,3)
x+y
ls()
rm(x,y)
ls()
install.packages("missForest")
install.packages("tidyverse")
install.packages("tensorflow")
install.packages("kerasR")
install.packages("keras")
install.packages("RTextTools")
library(pROC)
library(tidyverse)
library(lubridate)
library(RTextTools)
library(maxent)
set.seed(2)
##this pulls the full set of Trump's tweets between 2015 and 2018
url <- 'http://www.trumptwitterarchive.com/data/realdonaldtrump/%s.json'
all_tweets <- map(2015:2018, ~sprintf(url, .x)) %>%
map_df(jsonlite::fromJSON, simplifyDataFrame = TRUE) %>%
mutate(created_at = parse_date_time(created_at, "a b! d! H!:M!:S! z!* Y!")) %>%
tbl_df()
summary(all_tweets)
summary(all_tweets)
# segment into retweets and real tweets
retweets <- subset(all_tweets, startsWith(text, ".@") | startsWith(text,'"@') | startsWith(text,'RT'))
real_tweets <- subset(all_tweets, !(startsWith(text, ".@") | startsWith(text,'"@') | startsWith(text,'RT')))
#filter to only include tweets after he announced his candidacy
#also only include iPhone or Android tweets (there are a few misc others)
pol_trump <- subset(real_tweets, real_tweets$created_at >= "2015-06-16")
pol_trump <- subset(pol_trump, pol_trump$source == 'Twitter for iPhone' | pol_trump$source == 'Twitter for Android')
pol_trump$source <- ifelse(pol_trump$source == 'Twitter for iPhone', 'iPhone', 'Android')
table(pol_trump$source)
#these are the tweets that were created before the Android went away
#i.e. we can use the phone type as a proxy label in this period
labeled_inds <- pol_trump$created_at <= "2017-03-25"
#these are the tweets where we don't have a proxy label (i.e. after the Android stopped tweeting)
unlabeled_inds <- pol_trump$created_at >= "2017-03-26"
#create a feature matrix of trump tweet terms removing terms that do not appar in at least 1% of the docs
matrix <- create_matrix(pol_trump$text, language="english", removeSparseTerms=0.99, removeStopwords=TRUE, removeNumbers=TRUE, stemWords=TRUE, stripWhitespace=TRUE, toLower=TRUE)
#divide the feature matrix into labeled and unlabeled tweets based on the dates above
mat <- as.matrix(matrix)
labeled_tweets <- mat[labeled_inds,]
unlabeled_tweets <- mat[unlabeled_inds,]
#pull out the class labels
labels <- pol_trump$source[labeled_inds]
#divide the labeled tweets into training and test data
train_inds <- sample(nrow(labeled_tweets), .7*nrow(labeled_tweets))
train_x <- labeled_tweets[train_inds,]
test_x <- labeled_tweets[-train_inds,]
train_y <- labels[train_inds]
test_y <- labels[-train_inds]
#train a model to predict the class label (iPhone vs. Android)
model <- maxent(train_x, train_y)
## Look at model weights
model_weights=model@weights
## Consider the first 10 weights:
model_weights[1:10,]
## What is Feature 17?
colnames(train_x)[17]
m17 <- subset(model_weights,model_weights[,3] == 17)
## What is Feature 17's relationship to tweeting from the Android? iPhone?
## How about Feature 171?
colnames(train_x)[171]
m171 <- subset(model_weights,model_weights[,3] == 171)
#evaluate the predictions in terms of accuracy and AUC
predictions <- predict(model, test_x)
table(test_y, as.factor(predictions[,1]), dnn=c("actual", "predicted"))
recall_accuracy(test_y, predictions)
#get the AUC
test_y_num <- ifelse(test_y == 'Android', 1,0)
pred_num <- as.numeric(predictions[,3])
roc_obj <- roc(test_y_num, pred_num)
auc(roc_obj)
# 68 tweets predicted as iPhone but actually Android. What are those tweets?
not_iphone <- test_x[(test_y=="Android" & test_y!=as.factor(predictions[,1])),]
mislabeled_tweets=rownames(not_iphone)
## For example, compare tweet #1 to tweet #67
mislabeled_tweets[1]
mislabeled_tweets[67]
term.freq <- colSums(as.matrix(not_iphone))
plot(roc_obj)
## Check any word that appears at least 5 times
term.freq <- subset(term.freq, term.freq>=5)
term.freq=sort(term.freq)
df2 <- data.frame(term=names(term.freq), freq = term.freq)
ggplot(df2, aes(x=term,y=freq)) + geom_bar(stat="identity") + xlab("Terms")+ylab("Count") + coord_flip()
## In descending order
df2 <- transform(df2, term=reorder(term,freq))
ggplot(df2, aes(x=term,y=freq)) + geom_bar(stat="identity") + xlab("Terms")+ylab("Count") + coord_flip()
#look at the predicted labels on the unlabeled (recent) tweets
#which are the most likely to be Trump tweets?
#which are the most likely to be campaign tweets?
predictions_use <- as.numeric(predict(model, unlabeled_tweets)[,3])
unlabeled_original <- pol_trump[unlabeled_inds,]
unlabeled_original$prob <- predictions_use
#sort to see most vs. least likely
unlabeled_sorted <- unlabeled_original[order(unlabeled_original$prob),]
#go back and look at the misclassified tweets. what do you think was the cause of their misclassifications?
#go back and look at the misclassified tweets. what do you think was the cause of their misclassifications?
#try to improve your classifications by changing the featurization.
#go back and look at the misclassified tweets. what do you think was the cause of their misclassifications?
#try to improve your classifications by changing the featurization.
#OR, can you improve it with more historical tweets?
View(unlabeled_sorted)
load("C:/Users/Suvrodeep/OneDrive/Documents/UMD documents/Semester X/Data Mining - Paulson/Project/.RData")
rm(list = ls())
load("~/UMD documents/Semester X/Data Mining - Paulson/Project/final.RData")
library(tidyverse)
library(readr)
library(corrplot)
library(stringr)
library(quanteda)
library(missForest)
library(doParallel)
library(modeest)
library(randomForest)
library(caret)
library(xgboost)
library(h2o)
library(tensorflow)
library(tfestimators)
View(importance)
which(importance$Feature == "transit")
test <- select(full[which(full$Data_Type == "Test"),], -one_of(c("Obs", "review_scores_rating",
"high_booking_rate",
"Data_Type")))
cols.not_used <- c("is_location_exact", "house_rules", "access", "neighbourhood",
"host_neighbourhood", "host_has_profile_pic", "neighborhood_overview",
"space", "host_identity_verified")
test <- select(test, -one_of(cols.not_used))
test <- droplevels.data.frame(test)
library(readr)
library(randomForest)
library(ggplot2)
library(caret)
df <- read_csv("data.csv", col_names = TRUE)
df <- df[-which(df$Steering <= 0 | df$Throttle <= 0), ]
write_csv(df, "data_cleaned.csv")
setwd("~/GitHub/The-Enigma-Project-A-self-learning-1-10-scale-autonomous-Rock-Crawler/raspberry pi")
df <- read_csv("data.csv", col_names = TRUE)
df <- df[-which(df$Steering <= 0 | df$Throttle <= 0), ]
write_csv(df, "data_cleaned.csv")
ggplot(data = df, aes(Throttle)) + geom_histogram(bins = 200)
ggplot(data = df, aes(Throttle)) + geom_histogram(bins = 50)
ggplot(data = df, aes(Throttle)) + geom_histogram(bins = 100)
ggplot(data = df, aes(Steering)) + geom_histogram(bins = 100)
test.index <- createDataPartition(df$Steering, p=0.2, list = FALSE)
df.test <- df[test.index,]
df.train <- df[-test.index,]
#Random Forest
#Steering
#Train
model <- randomForest(Steering ~ . - Throttle, data = df.train)
pred <- predict(model, newdata = df.train)
df.comp <- data.frame(Actual = df.train$Steering, Residual = df.train$Steering - pred)
ggplot(data = df.comp, mapping = aes(Actual, Residual)) + geom_point() +
ggtitle("Steering (RandomForest Train)")
rmse <- sqrt(sum((df.comp$Residual ^ 2))/nrow(df))
print(rmse)
#Steering
#Test
pred <- predict(model, newdata = df.test)
df.comp <- data.frame(Actual = df.test$Steering, Residual = df.test$Steering - pred)
ggplot(data = df.comp, mapping = aes(Actual, Residual)) + geom_point() +
ggtitle("Steering (RandomForest Test)")
rmse <- sqrt(sum((df.comp$Residual ^ 2))/nrow(df.test))
print(rmse)
#Throttle
#Train
model <- randomForest(Throttle ~ . - Steering, data = df.train)
pred <- predict(model, newdata = df.train)
df.comp <- data.frame(Actual = df.train$Throttle, Residual = df.train$Throttle - pred)
ggplot(data = df.comp, mapping = aes(Actual, Residual)) + geom_point() +
ggtitle("Throttle (RandomForest Train)")
rmse <- sqrt(sum((df.comp$Residual ^ 2))/nrow(df))
print(rmse)
#Throttle
#Test
pred <- predict(model, newdata = df.test)
df.comp <- data.frame(Actual = df.test$Throttle, Residual = df.test$Throttle - pred)
ggplot(data = df.comp, mapping = aes(Actual, Residual)) + geom_point() +
ggtitle("Throttle (RandomForest Test)")
rmse <- sqrt(sum((df.comp$Residual ^ 2))/nrow(df.test))
print(rmse)
#Linear regression
#Steering
#Train
model <- lm(Steering ~ . - Throttle, data = df.train)
pred <- predict(model, newdata = df.train)
df.comp <- data.frame(Actual = df.train$Steering, Residual = df.train$Steering - pred)
ggplot(data = df.comp, mapping = aes(Actual, Residual)) + geom_point()
rmse <- sqrt(sum((df.comp$Residual ^ 2))/nrow(df))
print(rmse)
#Steering
#Test
pred <- predict(model, newdata = df.test)
df.comp <- data.frame(Actual = df.test$Steering, Residual = df.test$Steering - pred)
ggplot(data = df.comp, mapping = aes(Actual, Residual)) + geom_point()
rmse <- sqrt(sum((df.comp$Residual ^ 2))/nrow(df))
print(rmse)
rmse <- sqrt(sum((df.comp$Residual ^ 2))/nrow(df.test))
print(rmse)
#Random Forest
#Steering
#Train
model <- randomForest(Steering ~ . - Throttle, data = df.train)
pred <- predict(model, newdata = df.train)
df.comp <- data.frame(Actual = df.train$Steering, Residual = df.train$Steering - pred)
rmse <- sqrt(sum((df.comp$Residual ^ 2))/nrow(df.train))
print(rmse)
#Steering
#Test
pred <- predict(model, newdata = df.test)
df.comp <- data.frame(Actual = df.test$Steering, Residual = df.test$Steering - pred)
rmse <- sqrt(sum((df.comp$Residual ^ 2))/nrow(df.test))
print(rmse)
#Throttle
#Train
model <- randomForest(Throttle ~ . - Steering, data = df.train)
pred <- predict(model, newdata = df.train)
df.comp <- data.frame(Actual = df.train$Throttle, Residual = df.train$Throttle - pred)
rmse <- sqrt(sum((df.comp$Residual ^ 2))/nrow(df.train))
print(rmse)
#Throttle
#Test
pred <- predict(model, newdata = df.test)
df.comp <- data.frame(Actual = df.test$Throttle, Residual = df.test$Throttle - pred)
rmse <- sqrt(sum((df.comp$Residual ^ 2))/nrow(df.test))
print(rmse)
#Linear regression
#Steering
#Train
model <- lm(Steering ~ . - Throttle, data = df.train)
pred <- predict(model, newdata = df.train)
df.comp <- data.frame(Actual = df.train$Steering, Residual = df.train$Steering - pred)
ggplot(data = df.comp, mapping = aes(Actual, Residual)) + geom_point()
rmse <- sqrt(sum((df.comp$Residual ^ 2))/nrow(df.train))
print(rmse)
#Linear regression
#Steering
#Train
model <- lm(Steering ~ . - Throttle, data = df.train)
pred <- predict(model, newdata = df.train)
df.comp <- data.frame(Actual = df.train$Steering, Residual = df.train$Steering - pred)
ggplot(data = df.comp, mapping = aes(Actual, Residual)) + geom_point() +
ggtitle("Steering (Linear regression Train)")
rmse <- sqrt(sum((df.comp$Residual ^ 2))/nrow(df.train))
print(rmse)
#Steering
#Test
pred <- predict(model, newdata = df.test)
df.comp <- data.frame(Actual = df.test$Steering, Residual = df.test$Steering - pred)
ggplot(data = df.comp, mapping = aes(Actual, Residual)) + geom_point() +
ggtitle("Steering (Linear regression Test)")
rmse <- sqrt(sum((df.comp$Residual ^ 2))/nrow(df.test))
print(rmse)
#Throttle
#Train
model <- lm(Throttle ~ . - Steering, data = df.train)
pred <- predict(model, newdata = df.train)
df.comp <- data.frame(Actual = df.train$Throttle, Residual = df.train$Throttle - pred)
ggplot(data = df.comp, mapping = aes(Actual, Residual)) + geom_point() +
ggtitle("Throttle (Linear regression Train)")
rmse <- sqrt(sum((df.comp$Residual ^ 2))/nrow(df.train))
print(rmse)
#Throttle
#Test
model <- lm(Throttle ~ . - Steering, data = df.train)
pred <- predict(model, newdata = df.train)
df.comp <- data.frame(Actual = df.test$Throttle, Residual = df.test$Throttle - pred)
ggplot(data = df.comp, mapping = aes(Actual, Residual)) + geom_point() +
ggtitle("Throttle (Linear regression Test)")
rmse <- sqrt(sum((df.comp$Residual ^ 2))/nrow(df.test))
print(rmse)
#Throttle
#Test
model <- lm(Throttle ~ . - Steering, data = df.train)
pred <- predict(model, newdata = df.train)
df.comp <- data.frame(Actual = df.test$Throttle, Residual = df.test$Throttle - pred)
#Throttle
#Test
pred <- predict(model, newdata = df.test)
df.comp <- data.frame(Actual = df.test$Throttle, Residual = df.test$Throttle - pred)
gc()
df <- read_csv("data.csv", col_names = TRUE)
df <- df[-which(df$Steering <= 0 | df$Throttle <= 0), ]
write_csv(df, "data_cleaned.csv")
ggplot(data = df, aes(Throttle)) + geom_histogram(bins = 100)
ggplot(data = df, aes(Steering)) + geom_histogram(bins = 100)
test.index <- createDataPartition(df$Steering, p=0.2, list = FALSE)
df.test <- df[test.index,]
df.train <- df[-test.index,]
#Random Forest
#Steering
#Train
model <- randomForest(Steering ~ . - Throttle, data = df.train)
pred <- predict(model, newdata = df.train)
df.comp <- data.frame(Actual = df.train$Steering, Residual = df.train$Steering - pred)
ggplot(data = df.comp, mapping = aes(Actual, Residual)) + geom_point() +
ggtitle("Steering (RandomForest Train)")
rmse <- sqrt(sum((df.comp$Residual ^ 2))/nrow(df.train))
print(rmse)
#Steering
#Test
pred <- predict(model, newdata = df.test)
df.comp <- data.frame(Actual = df.test$Steering, Residual = df.test$Steering - pred)
ggplot(data = df.comp, mapping = aes(Actual, Residual)) + geom_point() +
ggtitle("Steering (RandomForest Test)")
rmse <- sqrt(sum((df.comp$Residual ^ 2))/nrow(df.test))
print(rmse)
#Throttle
#Train
model <- randomForest(Throttle ~ . - Steering, data = df.train)
pred <- predict(model, newdata = df.train)
df.comp <- data.frame(Actual = df.train$Throttle, Residual = df.train$Throttle - pred)
ggplot(data = df.comp, mapping = aes(Actual, Residual)) + geom_point() +
ggtitle("Throttle (RandomForest Train)")
rmse <- sqrt(sum((df.comp$Residual ^ 2))/nrow(df.train))
print(rmse)
#Throttle
#Test
pred <- predict(model, newdata = df.test)
df.comp <- data.frame(Actual = df.test$Throttle, Residual = df.test$Throttle - pred)
ggplot(data = df.comp, mapping = aes(Actual, Residual)) + geom_point() +
ggtitle("Throttle (RandomForest Test)")
rmse <- sqrt(sum((df.comp$Residual ^ 2))/nrow(df.test))
print(rmse)
#Linear regression
#Steering
#Train
model <- lm(Steering ~ . - Throttle, data = df.train)
pred <- predict(model, newdata = df.train)
df.comp <- data.frame(Actual = df.train$Steering, Residual = df.train$Steering - pred)
ggplot(data = df.comp, mapping = aes(Actual, Residual)) + geom_point() +
ggtitle("Steering (Linear regression Train)")
rmse <- sqrt(sum((df.comp$Residual ^ 2))/nrow(df.train))
print(rmse)
#Steering
#Test
pred <- predict(model, newdata = df.test)
df.comp <- data.frame(Actual = df.test$Steering, Residual = df.test$Steering - pred)
ggplot(data = df.comp, mapping = aes(Actual, Residual)) + geom_point() +
ggtitle("Steering (Linear regression Test)")
rmse <- sqrt(sum((df.comp$Residual ^ 2))/nrow(df.test))
print(rmse)
#Throttle
#Train
model <- lm(Throttle ~ . - Steering, data = df.train)
pred <- predict(model, newdata = df.train)
df.comp <- data.frame(Actual = df.train$Throttle, Residual = df.train$Throttle - pred)
ggplot(data = df.comp, mapping = aes(Actual, Residual)) + geom_point() +
ggtitle("Throttle (Linear regression Train)")
rmse <- sqrt(sum((df.comp$Residual ^ 2))/nrow(df.train))
print(rmse)
#Throttle
#Test
pred <- predict(model, newdata = df.test)
df.comp <- data.frame(Actual = df.test$Throttle, Residual = df.test$Throttle - pred)
ggplot(data = df.comp, mapping = aes(Actual, Residual)) + geom_point() +
ggtitle("Throttle (Linear regression Test)")
rmse <- sqrt(sum((df.comp$Residual ^ 2))/nrow(df.test))
print(rmse)
